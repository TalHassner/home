---
title: "TextOCR: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text"
collection: publications
permalink: /publication/2021_CVPR_4
excerpt: '[<font color="SkyBlue"><i>arXiv preprint</i></font>](https://arxiv.org/abs/2105.05486)'
date: 2021-06-16
venue: 'CVF/IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Nashville, TN'
paperurl: ''
citation: 'Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. <i> TextOCR: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text.</i> CVF/IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, 2021.'
---

<img src='../projects/TextOCR/teaser.png'><br/>
<b>TextOCR visualizations.</b> The figure shows diversity and density in TextOCR images.



### Abstract
A crucial component for the scene text based reasoning required for TextVQA and TextCaps datasets involve detecting and recognizing text present in the images using an optical character recognition (OCR) system. The current systems are crippled by the unavailability of ground truth text annotations for these datasets as well as lack of scene text detection and recognition datasets on real images disallowing the progress in the field of OCR and evaluation of scene text based reasoning in isolation from OCR systems. In this work, we propose TextOCR, an arbitrary-shaped scene text detection and recognition with 900k annotated words collected on real images from TextVQA dataset. We show that current state-of-the-art text-recognition (OCR) models fail to perform well on TextOCR and that training on TextOCR helps achieve state-of-the-art performance on multiple other OCR datasets as well. We use a TextOCR trained OCR model to create PixelM4C model which can do scene text based reasoning on an image in an end-to-end fashion, allowing us to revisit several design choices to achieve new state-of-the-art performance on TextVQA dataset.


[arXiv preprint](https://arxiv.org/abs/2105.05486)

[Bibtex](../projects/TextOCR/BibTeX.txt)

[Data and code](https://textvqa.org/textocr)
