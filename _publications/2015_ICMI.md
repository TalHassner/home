---
title: "Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns"
collection: publications
permalink: /publication/2015_ICMI
excerpt: '[<font color="SkyBlue"><i>Download paper</i></font>](../projects/cnn_emotions/LeviHassner_ICMI15.pdf)'
date: 2015-10-01
venue: 'Proc. ACM International Conference on Multimodal Interaction (ICMI), Seattle'
paperurl: ''
citation: 'Gil Levi and Tal Hassner. <i>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns.</i> Proc. ACM International Conference on Multimodal Interaction (ICMI), Seattle, 2015.'
---

<img src='../projects/cnn_emotions/teaser.jpg'><br/>
Image intensities (left) are converted to Local Binary Pattern (LBP) codes (middle), shown here as grayscale values. We propose to map these values to a 3D metric space (right) in order to use them as input for Convolutional Neural Network (CNN) models. 3D codes in the right image are visualized as RGB colors.

### Abstract
We present a novel method for classifying emotions from static facial images. Our approach leverages on the recent success of Convolutional Neural Networks (CNN) on face recognition problems. Unlike the settings often assumed there, far less labeled data is typically available for training emotion classification systems. Our method is therefore designed with the goal of simplifying the problem domain by removing confounding factors from the input images, with an emphasis on image illumination variations. This, in an effort to reduce the amount of data required to effectively train deep CNN models. To this end, we propose novel transformations of image intensities to 3D spaces, designed to be invariant to monotonic photometric transformations. These are applied to CASIA Webface images which are then used to train an ensemble of multiple architecture CNNs on multiple representations. Each model is then fine-tuned with limited emotion labeled training data to obtain final classification models. Our method was tested on the Emotion Recognition in the Wild Challenge (EmotiW 2015), Static Facial Expression Recognition sub-challenge (SFEW) and shown to provide a substantial, 15.36% improvement over baseline results (40% gain in performance)\*.

\* These results were obtained without training on any benchmark for emotion recognition other than the EmotiW'15 challenge benchmark. To our knowledge, to date, these are the highest results obtained under such circumstances.

[Download paper here](../projects/cnn_emotions/LeviHassner_ICMI15.pdf)

[BibTeX](../projects/cnn_emotions/BibTeX.txt)

### Downloads
This page provides code and data to allow reproducing our results. If you find our code useful, please add suitable reference to our paper in your work. Downloads include:
- [Python notebook](https://github.com/GilLevi/AgeGenderDeepLearning/blob/master/EmotiW_Demo.ipynb) for example usage.
- Zip file [with code for converting RGB values to our mapped LBP codes](../projects/cnn_emotions/LBP_mapping_Matlab.zip).
- Link to public dropbox zip file with [trained CNN models](https://drive.google.com/open?id=0BydFau0VP3XSYk9ZVnVNd0ZvVk0). The models include VGG_S trained on RGB and the four mapped LBP-based representations described in the paper. Warning, ~2GB file!
- A [Gist page](https://gist.github.com/GilLevi/54aee1b8b0397721aa4b) for our trained models, is being uploded to the BVLC/Caffe [Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo).


### What's New 
- <b>20-November-2017: </b>
Fixed broken links to Python notebook and CNN models. 

- <b>14-December-2015: </b>
Git repository added with sample code, meta-data files and instructions. 

<br/>
<b>Copyright and disclaimer</b>
<br/>Copyright 2015, Gil Levi and Tal Hassner 
