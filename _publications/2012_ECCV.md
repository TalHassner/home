---
title: "Motion Interchange Patterns for Action Recognition in Unconstrained Videos"
collection: publications
permalink: /publication/2012_ECCV
excerpt: '[<font color="SkyBlue"><i>Download paper</i></font>](../projects/MIP/MIP_eccv12.pdf), [<font color="SkyBlue"><i>Data</i></font>](../projects/ASLAN/ASLAN-main.html)'
date: 2012-10-01
venue: 'European Conference on Computer Vision (ECCV), Firenze , Italy'
paperurl: ''
citation: 'Orit Kliper-Gross, Yaron Gurovich, Tal Hassner.<i>Motion Interchange Patterns for Action Recognition in Unconstrained Videos.</i> European Conference on Computer Vision (ECCV), Firenze , Italy, 2012.'
---

### Abstract
Action Recognition in videos is an active research field that is fueled by an acute need, spanning several application domains. Still, existing systems fall short of the applicationsâ€™ needs in real-world scenarios, where the quality of the video is less than optimal and the viewpoint is uncontrolled and often not static. In this paper, we consider the key elements of motion encoding and focus on capturing local changes in motion directions. In addition, we decouple image edges from motion edges using a suppression mechanism, and compensate for global camera motion by using an especially fitted registration scheme. Combined with a standard bag-of-words technique, our methods achieves state-of-the-art performance in the most recent and challenging benchmarks. 

[Download paper here](../projects/MIP/MIP_eccv12.pdf)

[BibTeX](../projects/MIP/BibTeX.txt)


Related projects
------
[The Action Similarity Labeling Challenge (ASLAN)](../projects/ASLAN/ASLAN-main.html)<br/>


| <img src='../projects/MIP/mip.jpg' width='300'> | <img src='../projects/MIP/alphaij.jpg' width='300'>   | 
|:--------:|:-------:|
| (a) | (b) |

**The Motion Interchange Patterns (MIP) representation**. (a) Our encoding is based on comparing two SSD scores computed between three patches from three consecutive frames. Relative to the location of the patch in the current frame, the location of the patch in the previous (next) frame is said to be in direction i (j); The angle between directions i and j is denoted alpha. (b) Illustrating the different motion patterns captured by different i and j values. Blue arrows represent motion from a patch in position i in the previous frame; red for the motion to the patch j in the next frame. Shaded diagonal strips indicate same alpha values.

### Results
Below are some results of the MIP representation, applied to recent Action Recognition benchmarks. For additional results and more details, please see the paper.

<table align="center">
  <tr>
    <td><b>System</b></td>
    <td colspan="2"><b>No CSML</b></td>
    <td colspan="2"><b>With CSML</b></td>
  </tr>
  <tr>
    <td></td>
    <td>Accuracy</td>
    <td>AUC</td>
    <td>Accuracy</td>
    <td>AUC</td>
  </tr>
  <tr>
    <td>LTP [Yeffet & Wolf 09]</td>
    <td>55.45 +- 0.6%</td>
    <td>57.2</td>
    <td>58.50 +- 0.7%</td>
    <td>62.4</td>
  </tr>
  <tr>
    <td>HOG [Laptev et al. 08]</td>
    <td>58.55 +- 0.8%</td>
    <td>61.59</td>
    <td>60.15 +- 0.6%	</td>
    <td>64.2</td>
  </tr>
  <tr>
    <td>HOF [Laptev et al. 08]</td>
    <td>56.82 +- 0.6%</td>
    <td>58.56</td>
    <td>58.62 +- 1.0%</td>
    <td>61.8</td>
  </tr>
  <tr>
    <td>HNF [Laptev et al. 08]</td>
    <td>58.67 +- 0.9%</td>
    <td>62.16</td>
    <td>57.20 +- 0.8%</td>
    <td>60.5</td>
  </tr>
  <tr>
    <td>MIP single channel alpha=0</td>
    <td>58.27 +- 0.6%</td>
    <td>61.7</td>
    <td>61.52 +- 0.8%</td>
    <td>66.5</td>
  </tr>
  <tr>
    <td>MIP single best channel alpha=1</td>
    <td>61.45 +- 0.8%</td>
    <td>66.1</td>
    <td>63.55 +- 0.8%</td>
    <td>69.0</td>
  </tr>
  <tr>
    <td>MIP w/o suppression</td>
    <td>61.67 +- 0.9%</td>
    <td>66.4</td>
    <td>63.17 +- 1.1%</td>
    <td>68.4</td>
  </tr>
  <tr>
    <td>MIP w/o motion compensation</td>
    <td>62.27 +- 0.8%</td>
    <td>66.4</td>
    <td>63.57 +- 1.0%</td>
    <td>69.5</td>
  </tr>
  <tr>
    <td>MIP w/o both</td>
    <td>60.43 +- 1.0%</td>
    <td>64.8</td>
    <td>63.08 +- 0.9%</td>
    <td>68.2</td>
  </tr>
  <tr>
    <td>MIP on stabilized clips</td>
    <td>59.73 +- 0.77%</td>
    <td>62.9</td>
    <td>62.30 +- 0.77%</td>
    <td>66.4</td>
  </tr>
  <tr>
    <td>MIP</td>
    <td>62.23 +- 0.8%</td>
    <td>67.5</td>
    <td>64.62 +- 0.8%</td>
    <td>70.4</td>
  </tr>
  <tr>
    <td>HOG+HOF+HNF</td>
    <td>60.88 +- 0.8%</td>
    <td>65.3</td>
    <td>63.12 +- 0.9%</td>
    <td>68.0</td>
  </tr>
  <tr>
    <td>HOG+HOF+HNF with OSSML<br/>[Kliper-Gross et al. 11]</td>
    <td>62.52 +- 0.8%</td>
    <td>66.6</td>
    <td>64.25 +- 0.7%</td>
    <td>69.1</td>
  </tr>
  <tr>
    <td>MIP+HOG+HOF+HNF</td>
    <td>64.27 +- 1.0%</td>
    <td>69.2</td>
    <td><b>65.45 +- 0.8%</b></td>
    <td><b>71.92</b></td>
  </tr>
</table>

**Table 1.** Comparison to previous results on the [ASLAN benchmark](../projects/ASLAN/ASLAN-main.html). The average accuracy and standard error on the ASLAN benchmark is given for a list of methods (see text for details). All HOG, HOF, and HNF results are taken from \[Kliper-Gross et al. 11, Kliper-Gross et al. 12].<br/><br/>


<table align="center">
  <tr>
    <td><b>System</b></td>
    <td><b>Original clips</b></td>
    <td><b>Stabilized clips</b></td>
  </tr>
  <tr>
    <td>HOG/HOF [Laptev et al. 08]</td>
    <td>20.44%</td>
    <td>21.96%</td>
  </tr>  		
  <tr>
    <td>C2 [Jhuang et al. 07]</td>
    <td>22.83%</td>
    <td>23.18%</td>
  </tr>
  <tr>
    <td>Action Bank [Sadanand & Corso 12]</td>
    <td>26.90%</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>MIP</td>
    <td><b>29.17%</b></td>
    <td>N/A</td>
  </tr>
</table>

**Table 2.** Comparison to previous results on the HMDB51 database. Since our method contains a motion compensation component, we tested our method on the more challenging unstabilized videos. Our method significantly outperforms the best results obtained by previous work.<br/><br/>

<table align="center">
  <tr>
    <td><b>System</b></td>
    <td><b>Splits clips</b></td>
    <td><b>LOgO</b></td>
  </tr>
  <tr>
    <td>HOG/HOF [Laptev et al. 08]</td>
    <td>47.9%</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>Action Bank [Sadanand & Corso 12]</td>
    <td>57.9%</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>MIP</td>
    <td><b>68.51%</b></td>
    <td><b>72.68%</b></td>
  </tr>
</table>

**Table 3.** Comparison to previous results on the UCF50 database. Our method significantly outperforms all reported methods.<br/><br/>

### MIP video descriptor for action recognition - Code
MATLAB code for computing the Motion Interchange Patterns (MIP) video descriptor is now available for [download here](../projects/MIP/MIPcode.zip). Please see the [ReadMe.txt](../projects/MIP/ReadMe.txt) for information on how to install and run the code.

If you use this code in your own work, please cite [our paper](../projects/MIP/BibTeX.txt).

<br/><b>Copyright and disclaimer</b>
<br/>Copyright 2012, Orit Kliper-Gross, Yaron Gurovich, Tal Hassner, and Lior Wolf

