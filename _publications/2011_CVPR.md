---
title: "Face Recognition in Unconstrained Videos with Matched Background Similarity"
collection: publications
permalink: /publication/2011_CVPR
excerpt: '[<font color="SkyBlue"><i>Download paper</i></font>](../projects/ytfaces/ytfaces_2011.pdf)'
date:  2011-06-01
venue: 'IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Colorado Springs'
paperurl: ''
citation: 'Lior Wolf, Tal Hassner, and Itay Maoz. <i>Face Recognition in Unconstrained Videos with Matched Background Similarity.</i> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Colorado Springs, 2011.'
---

<img src = '../projects/ytfaces/teaser.jpg'><br/>
<b>Most confident MBGS results (L2 norm, mean operator).</b>The Same/Not-Same labels are the ground truth labels, and the Correct/Incorrect labels indicate whether the method predicted correctly. For example, the top right quadrant displays sameperson pairs that were most confidently labeled as not-same.

### Abstract
Recognizing faces in unconstrained videos is a task of mounting importance. While obviously related to face recognition in still images, it has its own unique characteristics and algorithmic requirements. Over the years several methods have been suggested for this problem, and a few benchmark data sets have been assembled to facilitate its study. However, there is a sizable gap between the actual application needs and the current state of the art. In this paper we make the following contributions. (a) We present a comprehensive database of labeled videos of faces in challenging, uncontrolled conditions (i.e., ‘in the wild’), the ‘YouTube Faces’ database, along with benchmark, pairmatching tests1 . (b) We employ our benchmark to survey and compare the performance of a large variety of existing video face recognition techniques. Finally, (c) we describe a novel set-to-set similarity measure, the Matched Background Similarity (MBGS). This similarity is shown to considerably improve performance on the benchmark tests.


[Project](http://www.cs.tau.ac.il/~wolf/ytfaces/)

[Download paper here](../projects/ytfaces/ytfaces_2011.pdf)
