<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0050)https://www.openu.ac.il/home/hassner/projects/MIP/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1255">
    
    <title>Motion Interchange Patterns for Action Recognition in Unconstrained Videos</title>
    <meta content="Tal Hassner" name="author">
    <meta content="Motion Interchange Patterns for Action Recognition in Unconstrained Videos" name="description">
    <style type="text/css">
        .style5
        {
            font-family: Verdana;
        }
        .style19
        {
            font-size: small;
            font-weight: bold;
        }
        .style23
        {
            direction: ltr;
            text-align: justify;
        }
        .style24
        {
            font-size: medium;
        }
        .style25
        {
            font-size: medium;
            font-weight: bold;
        }
        .style27
        {
            width: 833px;
        }
        .style32
        {
            width: 170px;
            text-align: center;
            font-style: italic;
        }
        .style33
        {
            width: 171px;
            text-align: center;
            font-style: italic;
        }
        .style35
        {
            width: 170px;
            text-align: center;
            font-weight: bold;
            font-style: italic;
        }
        .style36
        {
            width: auto;
        }
        .style14
        {
            height: 294px;
            width: 461px;
        }
        .style37
        {
        }
        .style38
        {
            text-align: left;
            font-weight: bold;
        }
        .style39
        {
            text-align: center;
            font-weight: bold;
        }
        .style40
        {
            text-align: center;
        }
        .style41
        {
            text-align: left;
            font-weight: bold;
            width: 337px;
        }
        .style42
        {
            width: 337px;
        }
        .style43
        {
            text-align: center;
            font-weight: bold;
            width: 129px;
        }
        .style44
        {
            text-align: center;
            width: 129px;
        }
        .style45
        {
            text-align: justify;
        }
        .style46
        {
            font-size: small;
        }
    </style>
</head>
<body style="background-color: rgb(30, 30, 30);">
    <p>
        &nbsp;</p>
    <div align="center">
            <table id="AutoNumber2" style="background-color: rgb(238, 238, 238);
                width: 899px; height: 899px;" border="0" cellpadding="0" cellspacing="0">
                
                    <tbody><tr>
                        <td style="width: 100%; font-family: Verdana; text-align: center;">
                            <h1 style="text-align: center; font-weight: normal; " class="style5">
                                &nbsp;</h1>
                            <h1 style="text-align: center; font-weight: normal; ">
                                <strong>Motion Interchange Patterns for Action Recognition in Unconstrained 
                                Videos</strong></h1>
                                <table align="center" class="style27">
                                    <tbody><tr>
                                        <td class="style35">
                                            <h4>
                                                <a href="http://www.wisdom.weizmann.ac.il/~kliper/">Orit Kliper-Gross</a></h4>
                                        </td>
                                        <td class="style35">
                                            <h4>
                                                <a href="http://www.cs.tau.ac.il/~yarongur/">Yaron Gurovich</a></h4>
                                        </td>
                                        <td class="style35">
                                            <h4>
                                                <a href="https://www.openu.ac.il/home/hassner/" style="text-align: center">Tal Hassner</a></h4>
                                        </td>
                                        <td class="style35">
                                            <h4>
                                                <a href="http://www.cs.tau.ac.il/~wolf/">Lior Wolf</a></h4>
                                        </td>
                                    </tr>
                                    <tr>
                                        <td class="style32">
                                            The Weizmann Institute of Science</td>
                                        <td class="style32">
                                            Tel-Aviv University</td>
                                        <td class="style32">
                                            The Open University of Israel</td>
                                        <td class="style33">
                                            Tel-Aviv University</td>
                                    </tr>
                            </tbody></table>
                                <br>
                            <h3>
                                <a href="https://www.openu.ac.il/home/hassner/projects/MIP/#Mip_Presentation">New! Slides are now available</a><br>
                                <a href="https://www.openu.ac.il/home/hassner/projects/MIP/#MIP_distribution">New ! Code is now available </a>
                                </h3>
                            <blockquote class="style23">
                                <div style="text-align: justify; font-size: medium; font-family: Verdana;">
                                    <span class="style24"><strong>Abstract</strong>:</span> Action Recognition in 
                                    videos is an active research field that is fueled by an acute need, spanning 
                                    several application domains. Still, existing systems fall short of the 
                                    applications' needs in real-world scenarios, where the quality of the video is 
                                    less than optimal and the viewpoint is uncontrolled and often not static. In 
                                    this paper, we consider the key elements of motion encoding and focus on 
                                    capturing local changes in motion directions. In addition, we decouple image 
                                    edges from motion edges using a suppression mechanism, and compensate for global 
                                    camera motion by using an especially fitted registration scheme. Combined with a 
                                    standard bag-of-words technique, our methods achieves state-of-the-art 
                                    performance in the most recent and challenging benchmarks.
                                    <br>
                                    <br class="style24"><big><span class="style25">Reference</span><span class="style24">:&nbsp; </span></big><font face="Tahoma">O. Kliper-Gross, Y. 
                                    Gurovich, T. Hassner, and L. Wolf, <em>Motion Interchange Patterns for Action 
                                    Recognition in Unconstrained Videos,</em> European Conference on Computer Vision 
                                    (ECCV), Firenze, Italy, Oct 2012</font><font class="style19">
                                    <br>
                                    <br>
                                    </font>
                                    <font class="style46">
                                    <a href="https://www.openu.ac.il/home/hassner/projects/MIP/MIP_eccv12.pdf">Click here for the PDF</a></font><font class="style19"><font class="style46"> (Note: draft version)<br></font>
                                    </font>
                                    <font class="style46"> <a name="Mip_Presentation"></a><a href="https://www.openu.ac.il/home/hassner/projects/MIP/Mip_Presentation.pdf">Click here for PDF slides</a> 
                                    (light version, ~2.5Mb)<br>
                                    <a href="https://www.openu.ac.il/home/hassner/projects/MIP/MIP%20presentation.zip">Click here for PPTX slides</a> (zip includes 
                                    videos, ~86.5Mb)<br>
                                    <span class="style46"><a href="https://www.openu.ac.il/home/hassner/projects/MIP/BibTeX.txt">Click here for the BibTex</a></span></font>
                                    <br>
                                    <br>
                                    <strong>Related projects:</strong><br>
                                    <a href="https://www.openu.ac.il/home/hassner/data/ASLAN/ASLAN.html">The Action 
                                    Similarity Labeling Challenge (ASLAN)</a><br>
                                </div>
                                </blockquote>
                                <h3>
                                    Supplementary video</h3>
                                <iframe width="420" height="315" src="./project_files/FEauYq-YqCI.html" frameborder="0" allowfullscreen=""></iframe>
                                <br>
                                <br>
                                <hr style="width: 100%; height: 2px;">
                                <blockquote class="style23">
                                <table class="style36">
                                    <tbody><tr>
                                        <td align="center" class="style37">
                                            <img alt="" class="style14" src="./project_files/mip.jpg">&nbsp;</td>
                                        <td align="center">
                                            <img alt="" class="style14" src="./project_files/alphaij.jpg">&nbsp;</td>
                                    </tr>
                                    <tr>
                                        <td align="center" class="style37">
                                            (a)&nbsp;</td>
                                        <td align="center">
                                            (b)&nbsp;</td>
                                    </tr>
                                    <tr>
                                    <td align="justify" colspan="2">
                                        <strong>The Motion Interchange Patterns (MIP) representation</strong>. (a) Our encoding is based on comparing two SSD scores computed between three patches from three consecutive frames. 
                                    Relative to the location of the patch in the current frame, the location of the patch in the previous (next) frame 
                                    is said to be in direction i (j); The angle between directions i and j is denoted alpha. (b) Illustrating the different 
                                    motion patterns captured by different i and j values. Blue arrows represent motion 
                                        <em>from</em> a patch in position i in the 
                                    previous frame; red for the motion <em>to</em> the patch j in the next frame. Shaded diagonal strips indicate same alpha values.
                                    </td>
                                    </tr>
                                </tbody></table>
                                </blockquote>
                                
                                <br>
                                <blockquote class="style23">
                                <hr style="width: 100%; height: 2px;">
                                    </blockquote>
                                <h3>
                                <br>
                                Results
                                <br>
                                </h3>
                                <blockquote class="style45">
                                Below are some results of the MIP 
                                    representation, applied to recent Action Recognition benchmarks. For additional results and more details, please see the paper. 
                                </blockquote>
                                <blockquote class="style23">
                                <table class="style36" ;="" rules="none" frame="box" border="2" align="center">
                                    <caption class="style45">
                                        <strong>Table 1.</strong> Comparison to previous results on the 
                                    <a href="https://www.openu.ac.il/home/hassner/data/ASLAN/ASLAN.html">ASLAN benchmark</a>. The average accuracy and standard error on the ASLAN benchmark is given for a list of methods (see text for details). All HOG, HOF, and HNF results are taken from 
                                    [Kliper-Gross et al. 11, Kliper-Gross et al. 12].
                                    &nbsp;</caption>
                                <tbody><tr>
                                <td class="style38">System</td><td class="style39" colspan="2"> No CSML</td> 
                                    <td class="style39" colspan="2"> With CSML </td>
                                </tr>
                                <tr>
                                <td>&nbsp;</td><td class="style40"> Accuracy</td><td class="style40">AUC </td>
                                    <td class="style40"> Accuracy </td><td class="style40"> AUC</td>
                                </tr>
                                <tr>
                                <td>LTP [Yeffet &amp; Wolf 09] </td><td class="style40">  55.45 +- 0.6% </td>
                                    <td class="style40">  57.2 </td><td class="style40">  58.50 +- 0.7% </td>
                                    <td class="style40">  62.4 </td>
                                </tr>
                                <tr>
                                <td>HOG [Laptev et al. 08] </td><td class="style40"> 58.55 +- 0.8% </td>
                                    <td class="style40"> 61.59 </td><td class="style40"> 60.15 +- 0.6% </td>
                                    <td class="style40"> 64.2 </td>
                                </tr>
                                <tr>
                                <td>HOF [Laptev et al. 08] </td><td class="style40"> 56.82 +- 0.6% </td>
                                    <td class="style40"> 58.56 </td><td class="style40"> 58.62 +- 1.0% </td>
                                    <td class="style40"> 61.8</td>
                                </tr>
                                <tr>
                                <td>HNF [Laptev et al. 08] </td><td class="style40"> 58.67 +- 0.9% </td>
                                    <td class="style40"> 62.16 </td><td class="style40"> 57.20 +- 0.8% </td>
                                    <td class="style40"> 60.5</td>
                                </tr>
                                <tr>
                                <td>MIP single channel alpha=0 </td><td class="style40"> 58.27 +- 0.6% </td>
                                    <td class="style40"> 61.7 </td><td class="style40"> 61.52 +- 0.8% </td>
                                    <td class="style40"> 66.5</td>
                                </tr>
                                <tr>
                                <td>MIP single best channel alpha=1 </td><td class="style40"> 61.45 +- 0.8% </td>
                                    <td class="style40"> 66.1 </td><td class="style40"> 63.55 +- 0.8% </td>
                                    <td class="style40"> 69.0</td>
                                </tr>
                                <tr>
                                <td>MIP w/o suppression </td><td class="style40"> 61.67 +- 0.9% </td>
                                    <td class="style40"> 66.4 </td><td class="style40"> 63.17 +- 1.1% </td>
                                    <td class="style40"> 68.4</td>
                                </tr>
                                <tr>
                                <td>MIP w/o motion compensation </td><td class="style40"> 62.27 +- 0.8% </td>
                                    <td class="style40"> 66.4 </td><td class="style40"> 63.57 +- 1.0% </td>
                                    <td class="style40"> 69.5</td>
                                </tr>
                                <tr>
                                <td>MIP w/o both </td><td class="style40"> 60.43 +- 1.0% </td><td class="style40"> 64.8 </td>
                                    <td class="style40"> 63.08 +- 0.9% </td><td class="style40"> 68.2</td>
                                </tr>
                                <tr>
                                <td>MIP on stabilized clips </td><td class="style40"> 59.73 +- 0.77% </td>
                                    <td class="style40"> 62.9 </td><td class="style40"> 62.30 +- 0.77% </td>
                                    <td class="style40"> 66.4</td>
                                </tr>
                                <tr>
                                <td>MIP </td><td class="style40"> 62.23 +- 0.8% </td><td class="style40"> 67.5 </td>
                                    <td class="style40"> 64.62 +- 0.8% </td><td class="style40"> 70.4</td>
                                </tr>
                                <tr>
                                <td>HOG+HOF+HNF </td><td class="style40"> 60.88 +- 0.8%  </td><td class="style40"> 65.3  </td>
                                    <td class="style40"> 63.12 +- 0.9%  </td><td class="style40"> 68.0</td>
                                </tr>
                                <tr>
                                <td>HOG+HOF+HNF with OSSML [Kliper-Gross et al. 11]  </td><td class="style40"> 62.52 +- 0.8%  </td>
                                    <td class="style40"> 66.6  </td><td class="style40"> 64.25 +- 0.7% </td>
                                    <td class="style40"> 69.1</td>
                                </tr>
                                <tr>
                                <td>MIP+HOG+HOF+HNF</td><td class="style40"> 64.27 +- 1.0%  </td>
                                    <td class="style40"> 69.2  </td><td class="style39"> 65.45 +- 0.8%  
                                    </td><td class="style39"> 71.92</td>
                                </tr>
                                </tbody></table>
                                </blockquote>
                                <blockquote class="style23">
                                <br><br>
                                <table class="style36" ;="" rules="none" frame="box" border="2" align="center">
                                    <caption style="text-align: justify">
                                        <strong>Table 2.</strong> Comparison to previous results on the HMDB51 database. Since our method 
                                contains a motion compensation component, we tested our method on the more challenging unstabilized videos. Our method significantly 
                                outperforms the best results obtained by previous work.
                                    &nbsp;</caption>
                                <tbody><tr>
                                <td class="style41">System</td><td class="style39"> Original clips </td> 
                                    <td class="style43"> Stabilized clips </td>
                                </tr>
                                <tr>
                                <td class="style42">HOG/HOF [Laptev et al. 08]</td><td class="style40"> 20.44%  </td>
                                    <td class="style44"> 21.96% </td>
                                </tr>
                                <tr>
                                <td class="style42">C2 [Jhuang et al. 07] </td><td class="style40"> 22.83%  </td>
                                    <td class="style44"> 23.18% </td>
                                </tr>
                                <tr>
                                <td class="style42">Action Bank [Sadanand &amp; Corso 12]  </td><td class="style40"> 26.90%  </td>
                                    <td class="style44"> N/A </td>
                                </tr>
                                <tr>
                                <td class="style42">MIP </td><td class="style40"> <strong>29.17%</strong> </td>
                                    <td class="style44"> N/A </td>
                                </tr>
                                </tbody></table>
                                                                <br><br>
                                </blockquote>
                                <blockquote class="style23">
                                <table class="style36" ;="" rules="none" frame="box" border="2" align="center">
                                    <caption style="text-align: justify">
                                        <strong>Table 3.</strong> Comparison to previous results on the UCF50 database. Our method significantly outperforms all reported methods.
                                    &nbsp;</caption>
                                <tbody><tr>
                                <td class="style41">System</td><td class="style39"> splits </td><td class="style43"> LOgO </td>
                                </tr>
                                <tr>
                                <td class="style42">HOG/HOF [Laptev et al. 08]</td><td class="style40"> 47.9%  </td>
                                    <td class="style44"> N/A </td>
                                </tr>
                                <tr>
                                <td class="style42">Action Bank [Sadanand &amp; Corso 12]  </td><td class="style40"> 57.9% </td>
                                    <td class="style44"> N/A </td>
                                </tr>
                                <tr>
                                <td class="style42">MIP </td><td class="style40"> <strong>68.51%</strong> </td>
                                    <td class="style44"> <strong>72.68%</strong> </td>
                                </tr>
                                </tbody></table>
                                </blockquote>
                                <br>
                                <blockquote class="style23">
                                <hr style="width: 100%; height: 2px;">

                            <h3 style="text-align: center">
                                <br>
                                <a name="MIP_distribution">MIP video descriptor for action recognition - Code</a>
                                 
                                <br>
                                </h3>
                                <blockquote class="style45">
                                    MATLAB code for computing the Motion Interchange Patterns (MIP) video descriptor is now available for
                                    <a href="https://www.openu.ac.il/home/hassner/projects/MIP/MIPcode.zip">download 
                                    here</a>. Please see the
                                    <a href="https://www.openu.ac.il/home/hassner/projects/MIP/ReadMe.txt">ReadMe.txt</a> 
                                    for information on how to install and run the code.<br>
                                    <br>
                                    If you use this code in your own work, please cite <a href="https://www.openu.ac.il/home/hassner/projects/MIP/BibTeX.txt">our 
                                    paper</a>.<br>
                                    <br>
                                    <strong>Copyright and disclaimer:</strong><br>
                                    Copyright 2012, Orit Kliper-Gross, Yaron Gurovich, Tal Hassner, and Lior Wolf<br>
                                    <br>
                                    The SOFTWARE ("MIPcode") is provided "as is", without any guarantee made as to 
                                    its suitability or fitness for any particular use. It may contain bugs, so use 
                                    of this tool is at your own risk. We take no responsibility for any damage that 
                                    may unintentionally be caused through its use. </blockquote>
                                <hr style="width: 100%; height: 2px;">

                                </blockquote>
                                <p>
                                Last update 27th of 
                                    Dec., 2012 
                                </p>
                            <p>
                                &nbsp;</p>
                                </td>
                                </tr>
                   
            </tbody></table>
    

    </div>
    
<script type="text/javascript">
	<!--
	try 
	{
		var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
		document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
	}
	catch (e) {}
	-->
</script><script src="./project_files/ga.js.download" type="text/javascript"></script>
<script type="text/javascript">
	<!--
	try 
	{
		var pageTracker = _gat._getTracker("UA-4552874-1");
		pageTracker._setDomainName("www.openu.ac.il");
		pageTracker._setAllowHash(false);
		pageTracker._trackPageview();
	} 
	catch (e) {}
	-->
</script>



</body></html>