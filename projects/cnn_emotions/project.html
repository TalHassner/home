<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1255">
    
    <title>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns</title>
    <meta content="Tal Hassner" name="author">
    <meta content="Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns" name="description">
    <style type="text/css">
        .style5
        {
            font-family: Verdana;
        }
        .style19
        {
            text-align: justify;
        }
        .style49
        {
            text-align: center;
        }
        .style23
        {
            text-align: justify;
        }
        .style52
        {
            direction: ltr;
        }
        .style54
        {
            font-weight: normal;
        }
        .style55
        {
            width: 216px;
        }
        .style56
        {
            color: #FF0000;
        }
    </style>
</head>
<body style="background-color: rgb(30, 30, 30);">
    <p>
        &nbsp;</p>
    <div align="center">
        <table id="AutoNumber2" style="background-color: rgb(238, 238, 238); width: 899px;
            height: 899px;" border="0" cellpadding="0" cellspacing="0">
            <tbody><tr>
                <td style="width: 100%; font-family: Verdana; text-align: center;">
                    <h1 style="text-align: center; font-weight: normal;" class="style5">
                        &nbsp;</h1>
                    <h2 style="text-align: center; font-weight: normal;">
                        <strong>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped 
                            Binary Patterns</strong></h2>
                    <table align="center" style="width: 443px">
                        <tbody><tr>
                            <td class="style55">
                                <h4 style="text-align: center">
                                    <a href="https://gilscvblog.wordpress.com/">Gil Levi</a><span class="style54"> <sup>
                                        1</sup></span></h4>
                            </td>
                            <td class="style48">
                                <h4 align="center">
                                    <a href="https://osnathassner.github.io/talhassner/" style="text-align: center">Tal Hassner</a>
                                    <span class="style54"><sup>1,2</sup></span></h4>
                            </td>
                        </tr>
                        <tr>
                            <td colspan="2" class="style49">
                                <sup>1</sup> <a href="https://www-e.openu.ac.il/">The Open University of Israel</a><br>
                                <sup>2</sup> <a href="http://www.isi.edu/home" style="text-align: center">USC / Information
                                    Sciences Institute</a>
                            </td>
                        </tr>
                    </tbody></table>
                    <br>
                    <img alt="Example of mapping from RGB (left) to LBP (mid) to the mapped LBP values used as input to the CNN (right)" longdesc="Example of mapping from RGB (left) to LBP (mid) to the mapped LBP values used as input to the CNN (right)" src="./teaser.png" style="height: 224px; width: 456px"><br>
                    <center>
                        <blockquote style="text-align: justify; width: 721px;">
                            <strong>Figure 1.</strong> Image intensities (left) are converted to Local Binary
                            Pattern (LBP) codes (middle), shown here as grayscale values. We propose to map
                            these values to a 3D metric space (right) in order to use them as input for Convolutional
                            Neural Network (CNN) models. 3D codes in the right image are visualized as RGB colors.</blockquote>
                    </center>
                    <blockquote class="style23">
                        <div style="text-align: justify; font-size: medium; font-family: Verdana;" class="style52">
                            <span class="style24"><strong>Abstract:</strong></span> We present a novel method
                            for classifying emotions from static facial images. Our approach leverages on the
                            recent success of Convolutional Neural Networks (CNN) on face recognition problems.
                            Unlike the settings often assumed there, far less labeled data is typically available
                            for training emotion classi cation systems. Our method is therefore designed with
                            the goal of simplifying the problem domain by removing confounding factors from
                            the input images, with an emphasis on image illumination variations. This, in an
                            effort to reduce the amount of data required to e ectively train deep CNN models.
                            To this end, we propose novel transformations of image intensities to 3D spaces,
                            designed to be invariant to monotonic photometric transformations. These are applied
                            to CASIA Webface images which are then used to train an ensemble of multiple architecture
                            CNNs on multiple representations. Each model is then ne-tuned with limited emotion
                            labeled training data to obtain nal classi cation models. Our method was tested
                            on the Emotion Recognition in the Wild Challenge (EmotiW 2015), Static Facial Expression
                            Recognition sub-challenge (SFEW) and shown to provide a substantial, 15.36% improvement
                            over baseline results (40% gain in performance)*.<br>
                            <br>
                            * These results 
                            were obtained without training on any benchmark for emotion recognition 
                            other than the EmotiW'15 challenge benchmark. To our knowledge, to date, 
                            these are the highest results obtained under such circumstances.<br>
                            <br class="style24">
                            <span class="style24"><strong>Reference:</strong></span> <font class="style19">Gil Levi
                                and Tal Hassner, <em>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped
                            Binary Patterns</em>, Proc. ACM International Conference on Multimodal Interaction (ICMI), Seattle, Nov. 2015<br>
                                <br>
                                <br>
                            </font><font class="style46">Click here for the <a href="https://osnathassner.github.io/talhassner/files/LeviHassnerICMI15.pdf">
                                PDF</a></font><font class="style19">
                                    <br>
                                </font><font class="style46"><span class="style46">Click here for the <a href="https://osnathassner.github.io/talhassner/projects/cnn_emotions/BibTeX.txt">
                                    BibTex</a></span></font>
                            <br>
                            <br>
                        </div>
                    </blockquote>
                    <hr style="width: 90%; height: 2px;">
                    <blockquote class="style23">
                        <div style="text-align: left; font-size: medium; font-family: Verdana; direction: ltr;">
                            <h3 style="text-align: center">
                                Downloads</h3>
                                This page provides code and data to allow reproducing our results. If you find our code useful, please add suitable reference to
                            our paper in your work. Downloads include:<br>
                            <ul>
                                <li><a href="https://github.com/GilLevi/AgeGenderDeepLearning/blob/master/EmotiW_Demo.ipynb">
                                    Python notebook</a> for example usage.</li>
                                <li>Zip file with <a href="https://osnathassner.github.io/talhassner/projects/cnn_emotions/LBP_mapping_Matlab.zip">code for converting RGB values to our mapped LBP codes</a>.</li>
                                <li>Link to public dropbox zip file with 
                                    <a href="https://drive.google.com/open?id=0BydFau0VP3XSYk9ZVnVNd0ZvVk0">trained CNN models</a>. The models include VGG_S trained on RGB and the four mapped LBP-based representations described in the paper. Warning, ~2GB file!</li>
                                <li>A <a href="https://gist.github.com/GilLevi/54aee1b8b0397721aa4b">Gist page</a> for our trained models, is being uploded to the BVLC/Caffe 
                                    <a href="https://github.com/BVLC/caffe/wiki/Model-Zoo">Model Zoo</a>.</li>
                            </ul>
                            <br>
                            <br>
                            <br>
                            <span class="style56"><strong>What's new</strong></span>
                            <br>
                            <br>
                            Nov. 20th, 2017: 
                            <br>
                            Fixed broken links to 
                            <a href="https://github.com/GilLevi/AgeGenderDeepLearning/blob/master/EmotiW_Demo.ipynb">Python notebook</a> and 
                            <a href="https://drive.google.com/open?id=0BydFau0VP3XSYk9ZVnVNd0ZvVk0">CNN models</a>. 
                            <br>
                            <br>
                            <br>
                            Dec. 14th, 2015: 
                            <br>
                            Git repository added with sample code, meta-data files and instructions. 
                            <br>
                            <br>
                            <br>
                            <br>
                            <br>
                            Copyright 2015, Gil Levi and Tal Hassner
                            <br>
                            <br>
                            The SOFTWARE provided in this page is provided "as is", without any guarantee made
                            as to its suitability or fitness for any particular use. It may contain bugs, so
                            use of this tool is at your own risk. We take no responsibility for any damage of
                            any sort that may unintentionally be caused through its use.
                            <br>
                            </div>
                            </blockquote>
                            &nbsp;<hr style="width: 90%; height: 2px;">
                            <p class="style49">
                                Last update 
                                14th of Dec., 2015</p>
                            <p>
                                &nbsp;</p>
                </td>
            </tr>
        </tbody></table>
    </div>
    
<script type="text/javascript">
	<!--
	try 
	{
		var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
		document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
	}
	catch (e) {}
	-->
</script><script src="./ga.js.download" type="text/javascript"></script>
<script type="text/javascript">
	<!--
	try 
	{
		var pageTracker = _gat._getTracker("UA-4552874-1");
		pageTracker._setDomainName("www.openu.ac.il");
		pageTracker._setAllowHash(false);
		pageTracker._trackPageview();
	} 
	catch (e) {}
	-->
</script>



</body></html>
