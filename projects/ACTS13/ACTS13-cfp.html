<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1255">
    
    <title>Action Similarity in Unconstrained Videos (ACTS'13)</title>
    <link rel="stylesheet" type="text/css" href="./style.css">
    
<script type="text/javascript">
	<!--
	try 
	{
		var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
		document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
	}
	catch (e) {}
	-->
</script><script src="./ga.js.download" type="text/javascript"></script>
<script type="text/javascript">
	<!--
	try 
	{
		var pageTracker = _gat._getTracker("UA-4552874-1");
		pageTracker._setDomainName("www.openu.ac.il");
		pageTracker._setAllowHash(false);
		pageTracker._trackPageview();
	} 
	catch (e) {}
	-->
</script>

</head>
<body>
    <div class="container">
        <div class="header">
            <a href="http://www.pamitc.org/cvpr13/">
                <img src="./logo.jpg" width="180" height="165" align="top"></a><!-- end .header --><a href="./ACTS13-main.html"><img src="./header.jpg" width="776" height="207"></a></div>
        <div class="sidebar1">
            <ul class="nav">
                <li><a href="./ACTS13-main.html">Home</a></li>
                <li><a href="./ACTS13-cfp.html">Call for Papers</a></li>
                <li><a href="./ACTS13-people.html">People</a></li>
                <li><a href="./ACTS13-submission.html">Submission</a></li>
                <li><a href="./ACTS13-program.html">Program</a></li>
                <li><a href="./ACTS13-links.html">Links</a></li>
            </ul>
            <p>&nbsp;</p>
            <p>
                <strong>Important dates</strong> <span class="style1"><u>Submission:</u></span><br>
                <del>March 30th, 2013</del>
                <br>
                <del>Apr. 6th, 2013</del>
                <br>
                Apr. 13th, 2013
                <br>
                11:59pm. EST
                <br>
                <span class="style1"><u>Notification:</u></span>
                <br>
                Apr. 25th, 2013<br>
                <span class="style1"><u>Camera ready:</u></span><br>
                May 3rd, 2013
                <br>
                <span class="style1"><u>Workshop:</u></span>
                <br>
                June 23rd, 2013
            </p>
            <p>&nbsp;</p>
            <p>&nbsp;</p>		
            <p>&nbsp;</p>
	    <p>&nbsp;</p>
            <p>&nbsp;</p>
	    <p>&nbsp;</p>
    	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
            <p>&nbsp;</p>
            <p>&nbsp;</p>		
            <p>&nbsp;</p>
	    <p>&nbsp;</p>
            <p>&nbsp;</p>
	    <p>&nbsp;</p>
    	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
            <!-- end .sidebar1 -->
        </div>
        <div class="content">
            <h1>
                Call for Papers</h1>
            <p>
                We solicit contributions in two categories.
            </p>
            <h3>
                A. Performance evaluation on the ASLAN benchmark</h3>
            <p>
                The Action Similarity LAbeliNg (ASLAN) benchmark focuses on pair-matching (same/not-same
                classification) of unconstrained video pairs of human actions. It is available,
                along with current state-of-the-art results, related code and information, from
                the following url: <a href="../ASLAN/ASLAN-main.html">ASLAN Data</a>.
            </p>
            <p>
                Submissions in this category should present new methods and demonstrate results
                on the ASLAN benchmark, compared to the existing state-of-the-art. By employing
                this standard benchmark we can compare alternative methods for action pair-matching
                and clearly identify those which outperform the rest.
            </p>
            <p>
                In this category, we accept also short papers (2 pages or less) presenting unpublished
                results of (possibly) previously published methods on the benchmark as described
                in its web-page. These results will be summarized and described by the organizers
                during the workshop. Authors may give a short description of their methods or refer
                to existing publications which give the details of the algorithms used. Short papers
                will not appear as separate publications in the workshop proceedings, but will be
                described collectively in a single summary article describing results on the benchmark.
            </p>
            <p>
                Regular papers (of standard CVPR format and length) should provide details of the
                algorithms employed. The authors are strongly encouraged to provide a link to their
                implementation (or an executable), but this is not a requirement for submission.
            </p>
            <p class="style2">
                <strong>Exceptional papers / top performing methods on the ASLAN benchmark will be announced at the workshop. Awards sponsorded by Microsoft.</strong>
            </p>
            <br>
            <br>
            <h3>
                B. New directions and techniques in unconstrained Action Recognition in videos</h3>
            <p>
                This includes (but is not limited to) papers concerning the following topics:
            </p>
            <p>
                <strong>Recognition-learning related:</strong></p>
            <ul>
                <li>Representations of actions in videos</li>
                <li>(dis-)similarity techniques for action representations</li>
                <li>Fine-grained Action Recognition</li>
                <li>Use of background samples and attributes for Action Recognition</li>
                <li>Action pair-matching vs. multi-class Action Recognition</li>
                <li>Action Detection in time and in space</li>
                <li>Similarity ranking of actions</li>
            </ul>
            <p>
                <strong>Human vision related</strong></p>
            <ul>
                <li>Human vision inspired Action Recognition</li>
            </ul>
            <p>
                <strong>Video processing/bottom-up feature related</strong></p>
            <ul>
                <li>Video stabilization methods for better Action Recognition</li>
            </ul>
            <p>
                Although we encourage papers in this category to include performance evaluations
                on the Action Similarity LAbeliNg challenge (ASLAN), we welcome also papers that
                will use ASLAN in other ways.
            </p>
            <!-- end .content -->
        </div>
        <div class="footer">
            <p>
                </p><center>
                    Copyright Â© The Open University of Israel 2013. All rights reserved.</center>
            <p></p>
            <!-- end .footer -->
        </div>
        <!-- end .container -->
    </div>


</body></html>
